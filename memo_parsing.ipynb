{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZqxAuI8dtdKZriola7LYa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tymoshenkoanna/memo_parsing/blob/main/memo_parsing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup as bs4\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "core_url = 'https://ru.openlist.wiki/'\n",
        "search_url = 'https://ru.openlist.wiki/%D0%A1%D0%BB%D1%83%D0%B6%D0%B5%D0%B1%D0%BD%D0%B0%D1%8F:OlSearch?olsearch-name=*&olsearch-birth_min=&olsearch-birth_max=&olsearch-death_min=&olsearch-death_max=&olsearch-birthplace=&olsearch-liveplace=&olsearch-nationality=&olsearch-social=&olsearch-profession=&olsearch-deathplace=&olsearch-burialplace=&olsearch-body=&olsearch-categories=&olsearch-arrest_min=&olsearch-arrest_max=&olsearch-indictment=&olsearch-conviction_min=&olsearch-conviction_max=&olsearch-conviction-org=&olsearch-sentence=&olsearch-detentionplace=&olsearch-release_min=&olsearch-release_max=&olsearch-execution_min=&olsearch-execution_max=&olsearch-archive-case-number=&olsearch-run=1&olsearch-advform=1#OlSearch-results-caption'\n",
        "reqs = requests.get(search_url)\n",
        "bs4 = bs4(reqs.text, 'html.parser')\n",
        "\n",
        "#extract all links from the search_url, searching by <a> tag\n",
        "links_http = open('links_http.txt', 'w')\n",
        "for link in bs4.find_all('a'):\n",
        "    links_http.write(\"%s\\n\" % link)\n",
        "\n",
        "#remove <a href=... at the beginning of each link line\n",
        "with open('links_http.txt') as oldfile, open('links_http_cut.txt', 'w') as newfile:\n",
        "    for line in oldfile:\n",
        "         if ':' not in line:\n",
        "            sep_1 = '\"/'\n",
        "            cut_1 = line.split(sep_1, 1)[1]\n",
        "            newfile.write(cut_1)\n",
        "\n",
        "#remove end of each line after > sign, to leave only actual link         \n",
        "with open('links_http_cut.txt') as oldfile_2, open('links_final.txt', 'w') as newfile_2:\n",
        "  for line in oldfile_2:\n",
        "      if '>' in line and ')' in line:\n",
        "            sep_2 = '\">'\n",
        "            cut_2 = line.split(sep_2, 1)[0]\n",
        "            newfile_2.write(\"https://ru.openlist.wiki/%s\\n\" % cut_2)\n",
        "\n",
        "f = open('links_final.txt', 'r')\n",
        "file_contents = f.read()\n",
        "print (file_contents)\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gM2hqA9F-bbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download source code of each individual link, decode cyrillic as utf-8\n",
        "from bs4 import BeautifulSoup as bs4\n",
        "import requests\n",
        "\n",
        "source = requests.get('https://ru.openlist.wiki/H%D0%B8%D0%BA%D0%B8%D1%82%D1%8E%D0%BA_%D0%92%D0%B0%D0%BB%D0%B5%D1%80%D0%B8%D0%B9_%D0%90%D0%BB%D0%B5%D0%BA%D1%81%D0%B0%D0%BD%D0%B4%D1%80%D0%BE%D0%B2%D0%B8%D1%87_(1912)')  \n",
        "\n",
        "#memo_data = bs4(source.text, 'lxml')\n",
        "memo_data = bs4(source.text, 'lxml') #creates an object with all html data from the page\n",
        "#memo_data=bs4(source.content,\"html.parser\") same as above, just different syntax\n",
        "#<span> tag is an inline container used to mark up a part of a text, or a part of a document.\n",
        "#print(memo_data)\n",
        "#memo_data.text #returns a string stripped of any HTML tags and metadata.\n",
        "#memo_data.find_all('li') #\"b\" is the tag for bold text and that's the best parameter\n",
        "\n",
        "#part works\n",
        "#for tag in memo_data.find_all(\"li\"):\n",
        "  #print(tag)\n",
        "\n",
        "#part works\n",
        "#for tag in memo_data.find_all(\"li\"):\n",
        "#  print(\"{0}: {1}\".format(tag.name, tag.text))\n",
        "\n",
        "#part works\n",
        "#for tag_li in memo_data.find_all(\"li\"):\n",
        "#  for tag_b in tag_li.find_all(\"b\"):\n",
        "#    print(tag_li)\n",
        "\n",
        "\n",
        "for tag_li in memo_data.find_all(\"li\"):#extract all lines that start with <li> tag\n",
        "  for tag_b in tag_li.find_all(\"b\"):\n",
        "    pr_data = (\"{0}: {1}\".format(tag_li.name, tag_li.text))#extract all lines that include <b> tag\n",
        "    for line in pr_data:\n",
        "      sep_3 = ':  '#define separator by which line will be split --> \":  \" colon and two spaces\n",
        "      pr_data_notag = pr_data.split(sep_3, 1)[1]#defined new cleaned data\n",
        "      f_data_1 = pr_data_notag.replace(\": \",\":\") #remove not needed spaces afetr \":\"\n",
        "      final_data = f_data_1.replace(\", \",\",\") #remove not needed spaces after \",\"\n",
        "    print(final_data)\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "       "
      ],
      "metadata": {
        "id": "cmQXi3XXEwP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QTqmSPlBLGoq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
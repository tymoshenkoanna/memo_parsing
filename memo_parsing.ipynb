{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjVPEabVJrk8ITSUI2KjBi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tymoshenkoanna/memo_parsing/blob/main/memo_parsing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs4\n",
        "\n",
        "# 1. create a list of links from one url\n",
        "core_url = 'https://ru.openlist.wiki/'\n",
        "search_url = 'https://ru.openlist.wiki/%D0%A1%D0%BB%D1%83%D0%B6%D0%B5%D0%B1%D0%BD%D0%B0%D1%8F:OlSearch?olsearch-name=*&olsearch-birth_min=&olsearch-birth_max=&olsearch-death_min=&olsearch-death_max=&olsearch-birthplace=&olsearch-liveplace=&olsearch-nationality=&olsearch-social=&olsearch-profession=&olsearch-deathplace=&olsearch-burialplace=&olsearch-body=&olsearch-categories=&olsearch-arrest_min=&olsearch-arrest_max=&olsearch-indictment=&olsearch-conviction_min=&olsearch-conviction_max=&olsearch-conviction-org=&olsearch-sentence=&olsearch-detentionplace=&olsearch-release_min=&olsearch-release_max=&olsearch-execution_min=&olsearch-execution_max=&olsearch-archive-case-number=&olsearch-run=1&olsearch-advform=1#OlSearch-results-caption'\n",
        "reqs = requests.get(search_url)\n",
        "bsl = bs4(reqs.text, 'html.parser')\n",
        "\n",
        "#extract all links from the search_url, searching by <a> tag\n",
        "links_http = open('links_http.txt', 'w')\n",
        "for link in bsl.find_all('a'):\n",
        "    links_http.write(\"%s\\n\" % link)\n",
        "\n",
        "#remove <a href=... at the beginning of each link line\n",
        "with open('links_http.txt') as oldfile, open('links_http_cut.txt', 'w') as newfile:\n",
        "    for line in oldfile:\n",
        "         if ':' not in line:\n",
        "            sep_1 = '\"/'\n",
        "            cut_1 = line.split(sep_1, 1)[1]\n",
        "            newfile.write(cut_1)\n",
        "\n",
        "#remove end of each line after > sign, to leave only actual link         \n",
        "\n",
        "#DON'T DELETE, WORKING CODE. Below will extract all links from the saved file. Due to the amount of links on the list it's commented out, and just three links are saved as a list as PCO of the code\n",
        "#links_list=[]\n",
        "#with open('links_http_cut.txt') as oldfile_2, open('links_final.txt', 'w') as newfile_2:\n",
        "#  for line in oldfile_2:\n",
        "#      if '>' in line and ')' in line:\n",
        "#            sep_2 = '\">'\n",
        "#            cut_2 = line.split(sep_2, 1)[0]\n",
        "#            newfile_2.write(\"https://ru.openlist.wiki/%s\\n\" % cut_2)\n",
        "#            links_list.append(\"https://ru.openlist.wiki/%s\" % cut_2)\n",
        "            links_list = ['{0} '.format(elem) for elem in links_list] #using list comprehension https://docs.python.org/3/reference/expressions.html#list-displays\n",
        "f = open('links_final.txt', 'r')\n",
        "links_file = f.read()\n",
        "#print(links_file)\n",
        "#links \n",
        "links_list=['https://ru.openlist.wiki/(1888)                                                                                                ', 'https://ru.openlist.wiki/H%D0%B8%D0%BA%D0%B8%D1%82%D1%8E%D0%BA_%D0%92%D0%B0%D0%BB%D0%B5%D1%80%D0%B8%D0%B9_%D0%90%D0%BB%D0%B5%D0%BA%D1%81%D0%B0%D0%BD%D0%B4%D1%80%D0%BE%D0%B2%D0%B8%D1%87_(1912)                                                                                               ', 'https://ru.openlist.wiki/%D0%81_%D0%90%D0%BB%D0%B5%D0%BA%D1%81%D0%B0%D0%BD%D0%B4%D1%80_%D0%90%D0%BB%D0%B5%D0%BA%D1%81%D0%B0%D0%BD%D0%B4%D1%80%D0%BE%D0%B2%D0%B8%D1%87_(1911)                                                                                              ']\n",
        "print(links_list)\n",
        "\n",
        "#2. creating headers for the table\n",
        "source = requests.get('https://ru.openlist.wiki/H%D0%B8%D0%BA%D0%B8%D1%82%D1%8E%D0%BA_%D0%92%D0%B0%D0%BB%D0%B5%D1%80%D0%B8%D0%B9_%D0%90%D0%BB%D0%B5%D0%BA%D1%81%D0%B0%D0%BD%D0%B4%D1%80%D0%BE%D0%B2%D0%B8%D1%87_(1912)')  \n",
        "memo_data = bs4(source.text, 'lxml') #creates an object with all html data from the page\n",
        "\n",
        "column_names = []\n",
        "for tag_li in memo_data.find_all(\"li\"):#extract all lines that start with <li> tag\n",
        "  for tag_b in tag_li.find_all(\"b\"): #extract all lines that include <b> tag\n",
        "    pr_data = (\"{0}: {1}\".format(tag_li.name, tag_li.text))\n",
        "    #print(pr_data)\n",
        "    #for line in pr_data:\n",
        "    for line in pr_data.splitlines():\n",
        "      #print(line)\n",
        "      sep_3 = ':  '#define separator by which line will be split --> \":  \" colon and two spaces\n",
        "      #print(line)\n",
        "      pr_data_notag = pr_data.split(sep_3, 1)[1]#defined new cleaned data\n",
        "      f_data_1 = pr_data_notag.replace(\": \",\":\") #remove not needed spaces afetr \":\"\n",
        "      final_data = f_data_1.replace(\", \",\",\") #remove not needed spaces after \",\"\n",
        "      sep_table_header = ':'\n",
        "      table_header = final_data.split(sep_table_header, 1)[0]\n",
        "    column_names.append(table_header)\n",
        "\n",
        "print(column_names) #printing column names\n",
        "\n",
        "#3. extracting values and adding them to the table with column names, which were extracted in step 2 above\n",
        "person_data = []\n",
        "for link in links_list:\n",
        "    #print(link)\n",
        "    source_person = requests.get(link)\n",
        "    memo_data_person = bs4(source_person.text, 'lxml')\n",
        "    #print(memo_data_person)\n",
        "    for tag_li in memo_data_person.find_all(\"li\"):#extract all lines that start with <li> tag\n",
        "      #print(tag_li)\n",
        "      for tag_b in tag_li.find_all(\"b\"): #extract all lines that include <b> tag  \n",
        "        #print(tag_b)\n",
        "        pr_data = (\"{0}: {1}\".format(tag_li.name, tag_li.text))\n",
        "        #print(pr_data)\n",
        "        for line in pr_data.splitlines():\n",
        "          if \"li\" in line:\n",
        "            sep_3 = ':  '#define separator by which line will be split --> \":  \" colon and two spaces\n",
        "            pr_data_notag = pr_data.split(sep_3, 1)[1]\n",
        "            f_data_1 = pr_data_notag.replace(\": \",\":\") #remove not needed spaces afetr \":\"\n",
        "            final_data = f_data_1.replace(\", \",\",\") #remove not needed spaces after \",\"\n",
        "            p_data = final_data.split(':', 1)[1]\n",
        "            person_data.append(p_data)\n",
        "\n",
        "print(person_data)\n",
        "#4. Create a table\n",
        "#import numpy as np\n",
        "#from tabulate import tabulate\n",
        "#data= [person_data]\n",
        "#print(tabulate(data, headers=column_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHsvMjY7UEzM",
        "outputId": "85162e00-df17-4d21-f875-fc9945999eb8"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://ru.openlist.wiki/(1888)                                                                                                ', 'https://ru.openlist.wiki/H%D0%B8%D0%BA%D0%B8%D1%82%D1%8E%D0%BA_%D0%92%D0%B0%D0%BB%D0%B5%D1%80%D0%B8%D0%B9_%D0%90%D0%BB%D0%B5%D0%BA%D1%81%D0%B0%D0%BD%D0%B4%D1%80%D0%BE%D0%B2%D0%B8%D1%87_(1912)                                                                                               ', 'https://ru.openlist.wiki/%D0%81_%D0%90%D0%BB%D0%B5%D0%BA%D1%81%D0%B0%D0%BD%D0%B4%D1%80_%D0%90%D0%BB%D0%B5%D0%BA%D1%81%D0%B0%D0%BD%D0%B4%D1%80%D0%BE%D0%B2%D0%B8%D1%87_(1911)                                                                                              ']\n",
            "['Дата рождения', 'Место рождения', 'Пол', 'Национальность', 'Гражданство (подданство)', 'Профессия / место работы', 'Место проживания', 'Где и кем арестован', 'Дата ареста', 'Обвинение', 'Осуждение', 'Осудивший орган', 'Статья', 'Приговор', 'Дата реабилитации', 'Реабилитирующий орган', 'Комментарий к аресту', 'Источники данных']\n",
            "['1888\\xa0г.', 'д.Тараса Боханского района Бурят-Монгольской АССР', 'мужской', 'бурят', 'подданный Монгольской Народной Республики', 'высшее', 'доцент Ленинградского Восточного института', 'г. Ленинград,ул. Рылеева,д.20,кв.18', 'беспартийный', '6 января 1942\\xa0г.', 'в заключении,Вятлаг', '22 сентября 1937\\xa0г.', '19 февраля 1940\\xa0г.', 'Особое Совещание при НКВД СССР', '58-6,58-9,58-11', '8 лет ИТЛ', 'Вятлаг НКВД', '25 мая 1956\\xa0г.', 'Военный трибунал ЛВО', 'Книга памяти Бурятии', '1912\\xa0г.', 'ГССР,Аджарская АССР,г.Батуми', 'мужчина', 'русский', 'СССР', 'заключенный Карлаг (?)', 'Каз.ССР,Акмолинская обл.,г.Кокчетав.', 'Оперотдел Карлаг HКВД.', '15 октября 1942\\xa0г.', 'АСА в военное время (', '18 апреля 1943\\xa0г.', 'Акмолинский облсуд,18.04.1943', '58-10 УК РСФСР', '8 лет ИТЛ.', '20 сентября 1972\\xa0г.', 'Судебная коллегия ВС Каз.ССР.', 'Реабилитирована за отсутствием состава преступления.', 'Список репрессированных лиц,архивные уголовные дела на которых хранятся в КНБ РК (pomnirod.ru)', '1911\\xa0г.', 'Китай,Харбин', 'мужчина', '1934\\xa0г.', 'П-19905', 'ГА РФ,архивно-следственное дело']\n"
          ]
        }
      ]
    }
  ]
}
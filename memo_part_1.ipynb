
from urllib.request import urlopen
from bs4 import BeautifulSoup 
import re
import pandas as pd
import requests

core_url = 'https://ru.openlist.wiki/'
search_url = 'https://ru.openlist.wiki/%D0%A1%D0%BB%D1%83%D0%B6%D0%B5%D0%B1%D0%BD%D0%B0%D1%8F:OlSearch?olsearch-name=*&olsearch-birth_min=&olsearch-birth_max=&olsearch-death_min=&olsearch-death_max=&olsearch-birthplace=&olsearch-liveplace=&olsearch-nationality=&olsearch-social=&olsearch-profession=&olsearch-deathplace=&olsearch-burialplace=&olsearch-body=&olsearch-categories=&olsearch-arrest_min=&olsearch-arrest_max=&olsearch-indictment=&olsearch-conviction_min=&olsearch-conviction_max=&olsearch-conviction-org=&olsearch-sentence=&olsearch-detentionplace=&olsearch-release_min=&olsearch-release_max=&olsearch-execution_min=&olsearch-execution_max=&olsearch-archive-case-number=&olsearch-run=1&olsearch-advform=1#OlSearch-results-caption'
reqs = requests.get(search_url)
bs4 = BeautifulSoup(reqs.text, 'html.parser')

#extract all links from the search_url, searching by <a> tag
links_http = open('links_http.txt', 'w')
for link in bs4.find_all('a'):
    links_http.write("%s\n" % link)

#remove <a href=... at the beginning of each link line
with open('links_http.txt') as oldfile, open('links_http_cut.txt', 'w') as newfile:
    for line in oldfile:
         if ':' not in line:
            sep_1 = '"/'
            cut_1 = line.split(sep_1, 1)[1]
            newfile.write(cut_1)

#remove end of each line after > sign, to leave only actual link         
with open('links_http_cut.txt') as oldfile_2, open('links_final.txt', 'w') as newfile_2:
  for line in oldfile_2:
      if '>' in line and ')' in line:
            sep_2 = '">'
            cut_2 = line.split(sep_2, 1)[0]
            newfile_2.write("https://ru.openlist.wiki/%s\n" % cut_2)

f = open('links_final.txt', 'r')
file_contents = f.read()
print (file_contents)
